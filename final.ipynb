{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio transformers torch pypdf2 sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Mistral model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647c1b2e3f3b4f39ab85927289b93edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translation model...\n"
     ]
    }
   ],
   "source": [
    "# Previous imports remain the same...\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import PyPDF2\n",
    "import torch\n",
    "import gradio as gr\n",
    "import os\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "# Device setup and model loading functions remain the same...\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize models with lower precision to save memory\n",
    "def load_models():\n",
    "    print(\"Loading Mistral model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        use_fast=True\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"Loading translation model...\")\n",
    "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "    tokenizer_ts = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_ts = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return tokenizer, model, tokenizer_ts, model_ts\n",
    "\n",
    "# Load models\n",
    "tokenizer, model, tokenizer_ts, model_ts = load_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://1b10f83c47016a49f2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1b10f83c47016a49f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def read_pdf_segments(file_path):\n",
    "    segments = []\n",
    "    try:\n",
    "        # Open the PDF file directly\n",
    "        pdf_reader = PyPDF2.PdfReader(file_path)\n",
    "        for page in pdf_reader.pages:\n",
    "            segment = page.extract_text()\n",
    "            if segment.strip():  # Only add non-empty segments\n",
    "                segments.append(segment)\n",
    "        return segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error in read_pdf_segments: {str(e)}\")  # Debug print\n",
    "        return [f\"Error reading PDF: {str(e)}\"]\n",
    "\n",
    "# generate_questions and translate_questions functions remain the same...\n",
    "def generate_questions(theme, segments) -> list:\n",
    "    resfin = []\n",
    "    \n",
    "    for segment in segments[:3]:\n",
    "        messages = [\n",
    "            {\n",
    "            \"role\": \"user\", \"content\":'''I will provide you with some text and a theme, your job is to identify the language of the provided text and theme, then generate a question based upon the theme in English. \n",
    "                                        Make sure the question you generate is answerable only by the provided text.\n",
    "                                        Please make no assumptions, and answer in English only.\n",
    "                                        In the case where no such question is possible, answer with the word NO only.''',\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\", \"content\":\"Sure, please provide me with the text and the theme.\"\n",
    "            },\n",
    "\n",
    "            {\n",
    "            \"role\": \"user\", \"content\":\"The text is demarkated by triple backticks:```होरी एक गरीब किसान था, जो मुश्किल से गुजारा कर पा रहा था। अपनी कठिनाइयों के बावजूद, वह एक गाय खरीदने का सपना देखता था, जो गाँव में समृद्धि का प्रतीक थी।```. The theme is demarkated by double backticks:``होरी की गाय खरीदने की इच्छा``.\",\n",
    "            },\n",
    "\n",
    "            {\"role\":\"assistant\",\"content\": \"Why did Hori want a cow?\"\n",
    "            },\n",
    "            {\n",
    "            \"role\":\"user\",\"content\": f'''The text is demarkated by triple backticks:```{segment}```. \n",
    "                                        The theme is demarkated by double backticks:``{theme}``.''',\n",
    "            }\n",
    "        ] \n",
    "\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "                model_inputs = encodeds.to(device)\n",
    "                \n",
    "                generated_ids = model.generate(\n",
    "                    model_inputs, \n",
    "                    max_new_tokens=1000, \n",
    "                    do_sample=True,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                decoded = tokenizer.batch_decode(generated_ids)\n",
    "                res = decoded[0]\n",
    "            \n",
    "            last_inst_index = res.rfind('[/INST]')\n",
    "            if last_inst_index != -1:\n",
    "                res = res[last_inst_index + len('[/INST]'):]\n",
    "                \n",
    "            question_mark_index = res.find('?')\n",
    "            if question_mark_index != -1:\n",
    "                res = res[:question_mark_index + 1]\n",
    "            \n",
    "            if \"NO\" not in res.strip():\n",
    "                resfin.append(res)\n",
    "            if len(resfin) == 3:\n",
    "                break\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "            continue\n",
    "\n",
    "    return resfin if resfin else [\"No questions could be generated. Try with a different theme or text.\"]\n",
    "def validate_question(question, segments):\n",
    "    resfin = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": '''I will provide you with a question, and some text. \n",
    "                Your job is to tell me if the question that I provided is relevant to the given text or is incoherent. \n",
    "                Also if you find it relevant can u give the part of the text where you find its resemblance''',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": \"Sure, please provide me with the text and the question.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'''The text is demarkated by triple backticks:```{segment}```.\n",
    "                The question is demarkated by double backticks:``{question}``.''',\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "                model_inputs = encodeds.to(device)\n",
    "                \n",
    "                generated_ids = model.generate(\n",
    "                    model_inputs, \n",
    "                    max_new_tokens=1000, \n",
    "                    do_sample=True,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                decoded = tokenizer.batch_decode(generated_ids)\n",
    "                res = decoded[0]\n",
    "                \n",
    "                last_inst_index = res.rfind('[/INST]')\n",
    "                if last_inst_index != -1:\n",
    "                    res = res[last_inst_index + len('[/INST]'):]\n",
    "                \n",
    "                words_notexist = ['incoherent', 'irrelevant', 'not relevant', 'not coherent', \n",
    "                                'does not make sense', 'doesnt make sense', 'not related', \n",
    "                                'not coherent', 'not relevant', 'is not directly relevant']\n",
    "                \n",
    "                if all(word not in res.strip().lower() for word in words_notexist):\n",
    "                    resfin.append(res)\n",
    "                    break\n",
    "                    \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during validation: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return resfin[0] if resfin else \"Question is not relevant to the provided text.\"\n",
    "\n",
    "def translate_questions(questions, target_lang):\n",
    "    translated_texts = []\n",
    "    tokenizer_ts.src_lang = \"eng_Latn\"\n",
    "    \n",
    "    for question in questions:\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                inputs = tokenizer_ts(question, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                forced_bos_token_id = tokenizer_ts.convert_tokens_to_ids(target_lang)\n",
    "                translated_tokens = model_ts.generate(\n",
    "                    **inputs, \n",
    "                    forced_bos_token_id=forced_bos_token_id, \n",
    "                    max_new_tokens=50\n",
    "                )\n",
    "                translated_text = tokenizer_ts.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "                translated_texts.append(translated_text)\n",
    "        except Exception as e:\n",
    "            translated_texts.append(f\"Translation error: {str(e)}\")\n",
    "    \n",
    "    return translated_texts\n",
    "\n",
    "# Modified process_book function with proper file handling\n",
    "def process_book_generator(file_obj, theme):\n",
    "    if file_obj is None:\n",
    "        return \"Please upload a PDF file.\"\n",
    "    \n",
    "    try:\n",
    "        file_name = file_obj.name if hasattr(file_obj, 'name') else \"uploaded.pdf\"\n",
    "        segments = read_pdf_segments(file_obj)\n",
    "        \n",
    "        if not segments or (len(segments) == 1 and segments[0].startswith(\"Error\")):\n",
    "            return \"No text could be extracted from the PDF.\"\n",
    "        \n",
    "        questions = generate_questions(theme, segments)\n",
    "        if not questions:\n",
    "            return \"No questions could be generated.\"\n",
    "        \n",
    "        filename = file_name.lower()\n",
    "        if \"godan\" in filename or \"autobiography\" in filename:\n",
    "            questions = translate_questions(questions, \"hin_Deva\")\n",
    "        elif \"narayan-kwach\" in filename:\n",
    "            questions = translate_questions(questions, \"san_Deva\")\n",
    "        \n",
    "        return \"\\n\\n\".join(questions)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_book_generator: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "def process_book_validator(file_obj, question):\n",
    "    if file_obj is None:\n",
    "        return \"Please upload a PDF file.\"\n",
    "    \n",
    "    if not question:\n",
    "        return \"Please enter a question to validate.\"\n",
    "    \n",
    "    try:\n",
    "        segments = read_pdf_segments(file_obj)\n",
    "        \n",
    "        if not segments or (len(segments) == 1 and segments[0].startswith(\"Error\")):\n",
    "            return \"No text could be extracted from the PDF.\"\n",
    "        \n",
    "        validation_result = validate_question(question, segments)\n",
    "        return validation_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_book_validator: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "def create_interfaces():\n",
    "    # Question Generator Interface\n",
    "    generator_interface = gr.Interface(\n",
    "        fn=process_book_generator,\n",
    "        inputs=[\n",
    "            gr.File(label=\"Upload PDF\", file_types=[\".pdf\"]),\n",
    "            gr.Textbox(label=\"Enter theme\")\n",
    "        ],\n",
    "        outputs=gr.Textbox(label=\"Generated Questions\", lines=5),\n",
    "        title=\"Book Question Generator\",\n",
    "        description=\"Upload a PDF and specify a theme to generate relevant questions. The system will automatically detect the language based on the filename and translate questions if necessary.\"\n",
    "    )\n",
    "    \n",
    "    # Question Validator Interface\n",
    "    validator_interface = gr.Interface(\n",
    "        fn=process_book_validator,\n",
    "        inputs=[\n",
    "            gr.File(label=\"Upload PDF\", file_types=[\".pdf\"]),\n",
    "            gr.Textbox(label=\"Enter question to validate\")\n",
    "        ],\n",
    "        outputs=gr.Textbox(label=\"Validation Result\", lines=5),\n",
    "        title=\"Question Validator\",\n",
    "        description=\"Upload a PDF and enter a question to check if it's relevant to the text content.\"\n",
    "    )\n",
    "    \n",
    "    # Combine interfaces\n",
    "    return gr.TabbedInterface(\n",
    "        [generator_interface, validator_interface],\n",
    "        [\"Question Generator\", \"Question Validator\"]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    iface = create_interfaces()\n",
    "    iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
